# DeepSeek Model

A simple demonstration of using the DeepSeek R1 Distill Llama 8B model for text generation.

## What It Does

This notebook shows how to:
- Load a quantized DeepSeek model using LlamaModel
- Generate responses to prompts
- Display the output in Markdown format

## Requirements

- Python 3.x
- llamacpp_model (custom wrapper)
- IPython
- A compatible DeepSeek model file (.gguf format)

## Example Usage

The notebook demonstrates using the model to solve a step-by-step financial calculation problem.

## Model Information

Uses DeepSeek R1 Distill Llama 8B, a powerful and efficient language model that runs locally on your machine.
